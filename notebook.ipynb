{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34f97ef",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47555961",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0616386",
   "metadata": {},
   "source": [
    "## Physical Models of target and interceptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5630312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physics.missile import PhysicalMissleModel\n",
    "import physics.math as physics\n",
    "import numpy as np\n",
    "\n",
    "interceptor_speed = physics.mach_to_ms(4.0)  # Speed of the interceptor in m/s\n",
    "target_speed = physics.mach_to_ms(3.0)  # Speed of the target in m/s\n",
    "\n",
    "target_init_velocty = np.array([target_speed * 0.707, target_speed * 0.707, 0.0]) # in a 45 degree angle\n",
    "\n",
    "target = PhysicalMissleModel(velocity=target_init_velocty, max_acc=100 * 9.81, pos=np.array([-5_000.0, -5_000.0, 20_000.0]))\n",
    "interceptor = PhysicalMissleModel(velocity=np.array([0.0, 0.0, interceptor_speed]), max_acc=100 * 9.81, pos=np.array([0.0, 0.0, 100.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3072d3",
   "metadata": {},
   "source": [
    "## Environment & Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.environment import MissileEnv, MissileEnvSettings\n",
    "from pilots.random_evasion_pilot import RandomEvasionPilot\n",
    "from pilots.constant_acc_pilot import ConstantAccelerationPilot\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# target behavior\n",
    "target_pilot = RandomEvasionPilot()\n",
    "# target_pilot = ConstantAccelerationPilot(acc=np.array([0.2, 0.0]))\n",
    "# target_pilot = None # keep trajectory\n",
    "\n",
    "settings = MissileEnvSettings()\n",
    "settings.time_step = 0.05    # Time step for the simulation\n",
    "settings.realtime = False    # Runs faster than real-time\n",
    "settings.time_limit = 50.0  # Time-limit for the episode\n",
    "env = MissileEnv(settings=settings, target=target, interceptor=interceptor, target_pilot=target_pilot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f42b1a",
   "metadata": {},
   "source": [
    "# Proportional Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67649d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.matplot_viz import MatplotVisualizer\n",
    "\n",
    "viz = MatplotVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32442557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pilots.proportional_nav_pilot import PlanarProportionalNavPilot, ZemProportionalNavPilot, SpaceProportionalNavPilot\n",
    "from visualization.matplot_viz import MatplotVisualizer\n",
    "import time\n",
    "\n",
    "\n",
    "# setup pilots\n",
    "proportional_nav_pilot = ZemProportionalNavPilot(max_acc=100*9.81, n=7)\n",
    "\n",
    "max_sim_time = 0.0\n",
    "episodes = 10\n",
    "env.uncertainty = 0.6\n",
    "env.set_current_agent_name(\"Prop. Nav. Pilot\")\n",
    "\n",
    "viz.reset()\n",
    "\n",
    "for i in range(episodes):\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # get un-normalized observations for the interceptor (environment outputs normalized observations for RL agent)\n",
    "        obs = env.get_interceptor_observations(settings.time_step).pack()\n",
    "\n",
    "        # pilot the interceptor using the proportional navigation algorithm\n",
    "        action = proportional_nav_pilot.step(obs, settings.time_step, interceptor, target)\n",
    "\n",
    "        obs, reward, done, _, _ = env.step(action)  # Take a step in the environment\n",
    "\n",
    "    # Record the best distance for this episode\n",
    "    max_sim_time = max(max_sim_time, env.sim_time)\n",
    "    viz.add_episode_data(env.current_episode)\n",
    "\n",
    "\n",
    "viz.render(max_sim_time)\n",
    "# viz.save_playback(\"prop-nav.gif\", max_sim_time, 5.0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.save_playback(\"./.playbacks/prop-nav.gif\", max_sim_time, 5.0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73102dc1",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce4163",
   "metadata": {},
   "source": [
    "## Soft Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98343373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "monitored_env = Monitor(env) # required for convergence check in callback\n",
    "\n",
    "# SAC model hyperparameters\n",
    "entropy_coef = \"auto\"  # Automatically adjust entropy coefficient\n",
    "target_entropy = \"auto\"  # Target entropy for the policy\n",
    "device = \"cuda\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ba891",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC(\"MlpPolicy\", monitored_env, \n",
    "            verbose=1, \n",
    "            tensorboard_log=\"./.logs/sac\", \n",
    "            device=device, \n",
    "            # ent_coef=entropy_coef, \n",
    "            # target_entropy=target_entropy\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if snapshot file exists\n",
    "snapshot_path = \"./.snapshots/snapshot-sac.zip\"\n",
    "if os.path.exists(snapshot_path):\n",
    "    model = SAC.load(snapshot_path, env=monitored_env, \n",
    "                     device=device, \n",
    "                     # ent_coef=entropy_coef, \n",
    "                     # target_entropy=target_entropy\n",
    "            )\n",
    "    print(\"Loaded existing model from snapshot.\")\n",
    "else:\n",
    "    print(\"No existing model snapshot found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_distance(episode):\n",
    "    # get the best distance between the interceptor and the target\n",
    "    return min([state.distance for time, state in episode.get_interceptor(env.current_agent_name).states.all.items()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db72a6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d13b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.convergence_callback import ConvergenceCallback\n",
    "from util.save_on_episode_end import SaveOnEpisodeEnd\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "# Train until desired results\n",
    "def eval_model(model):\n",
    "    eval_distances = []\n",
    "    for i in range(10):\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs = env.get_interceptor_observations(settings.time_step).pack()\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, _, done, _, _ = env.step(action)\n",
    "\n",
    "        best_distance = get_best_distance(env.current_episode)\n",
    "        eval_distances.append(best_distance)\n",
    "    \n",
    "    mean_distance = np.mean(eval_distances)\n",
    "    print(f\"Mean distance: {mean_distance:.2f} m\")\n",
    "\n",
    "    return mean_distance\n",
    "\n",
    "def train_until_miss_distance(model, model_name=\"rl\", distance=100.0):\n",
    "    best_mean_distance = float('inf')\n",
    "\n",
    "    complete = False\n",
    "    while not complete:    \n",
    "        with tqdm(total=20_000, desc=\"Training Progress\", unit=\"steps\") as pbar:\n",
    "            model.learn(total_timesteps=20_000, progress_bar=pbar)\n",
    "\n",
    "        mean_distance = eval_model(model)\n",
    "        \n",
    "        if mean_distance < best_mean_distance:\n",
    "            best_mean_distance = mean_distance\n",
    "            model.save(f\"./.snapshots/snapshot-{model_name}\")\n",
    "            print(f\"New best model saved with mean distance: {best_mean_distance:.2f} m\")\n",
    "\n",
    "        if mean_distance < distance:\n",
    "            complete = True\n",
    "            print(f\"Training complete! Mean distance: {mean_distance:.2f} m\")\n",
    "\n",
    "def train_until_convergence(model, model_name=\"rl\", reward_variance_threshold=200):\n",
    "    convergence_callback = ConvergenceCallback(10, reward_variance_threshold)\n",
    "    save_model_callback = SaveOnEpisodeEnd(save_dir=\"./.snapshots\", model_name=model_name)\n",
    "\n",
    "    callbacks = CallbackList([convergence_callback, save_model_callback])\n",
    "\n",
    "    max_timesteps = 1_000_000\n",
    "\n",
    "    with tqdm(total=max_timesteps, desc=\"Training Progress\", unit=\"steps\") as pbar:\n",
    "        model.learn(total_timesteps=max_timesteps, callback=callbacks, progress_bar=pbar)\n",
    "\n",
    "    model.save(f\"./.snapshots/converged-{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27663323",
   "metadata": {},
   "source": [
    "### Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.uncertainty = 0.1\n",
    "train_until_miss_distance(model, \"sac\", 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "def sample_sac_params(trial: optuna.Trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-1),\n",
    "        \"buffer_size\": trial.suggest_int(\"buffer_size\", 1_000, 1_000_000, step=50_000),\n",
    "        \"learning_starts\": trial.suggest_int(\"learning_starts\", 100, 10_000),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512, 1024]),\n",
    "        \"tau\": trial.suggest_float(\"tau\", 1e-3, 0.1),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.9, 0.999),\n",
    "        \"ent_coef\": trial.suggest_float(\"ent_coef\", 0.0, 0.2),\n",
    "        \"gradient_steps\": trial.suggest_int(\"gradient_steps\", 1, 20),\n",
    "    }\n",
    "\n",
    "# 2. Evaluation callback for pruning\n",
    "class TrialEvalCallback(EvalCallback):\n",
    "    def __init__(self, eval_env, trial, n_eval_episodes=5, eval_freq=5_000):\n",
    "        super().__init__(eval_env, eval_freq=eval_freq, n_eval_episodes=n_eval_episodes, verbose=0)\n",
    "        self.trial = trial\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if super()._on_step():\n",
    "            self.trial.report(self.last_mean_reward, self.n_calls)\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# 3. Objective function\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    total_timesteps = 50_000\n",
    "\n",
    "    monitored_env = Monitor(env)  # Required for convergence check in callback\n",
    "\n",
    "    kwargs = sample_sac_params(trial)\n",
    "    model = SAC(\"MlpPolicy\", monitored_env, verbose=0, tensorboard_log=\"./.logs/sac_optuna\", **kwargs)\n",
    "\n",
    "    print(f\"Trial {trial.number} parameters: {kwargs}\")\n",
    "\n",
    "    eval_callback = TrialEvalCallback(monitored_env, trial,\n",
    "                                      n_eval_episodes=5,\n",
    "                                      eval_freq=total_timesteps // 10)\n",
    "\n",
    "    try:\n",
    "        model.learn(total_timesteps=total_timesteps, callback=eval_callback)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72718e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=10, multivariate=True)\n",
    "pruner = MedianPruner(n_startup_trials=10, n_warmup_steps=1)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler, pruner=pruner)\n",
    "\n",
    "study.optimize(objective, n_trials=50, timeout=3600)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf65c8",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe121be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract reward components over time\n",
    "def create_reward_plot(infos):\n",
    "    times = list(infos.keys())\n",
    "    dist_rewards = [info[\"dist-reward\"] for info in infos.values()]\n",
    "    closing_rate_rewards = [info[\"closing-rate-reward\"] for info in infos.values()]\n",
    "    event_rewards = [info[\"event-reward\"] for info in infos.values()]\n",
    "    action_punishments = [info[\"action-punishment\"] for info in infos.values()]\n",
    "    ground_penalties = [info[\"ground-penality\"] for info in infos.values()]\n",
    "    rewards = [info[\"reward\"] for info in infos.values()]\n",
    "\n",
    "    # Create the plot\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(times, dist_rewards, label=\"Distance Reward\")\n",
    "    plt.plot(times, closing_rate_rewards, label=\"Closing Rate Reward\")\n",
    "    plt.plot(times, event_rewards, label=\"Event Reward\")\n",
    "    plt.plot(times, action_punishments, label=\"Action Punishment\")\n",
    "    plt.plot(times, ground_penalties, label=\"Ground Penalty\")\n",
    "    plt.plot(times, rewards, label=\"Total Reward\", linestyle='--', color='black', linewidth=2.5)\n",
    "\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Reward Components\")\n",
    "    plt.title(\"Reward Components Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def render_reward_plot(infos):\n",
    "    plt = create_reward_plot(infos)\n",
    "    plt.show()\n",
    "    plt.show(block=False)\n",
    "\n",
    "def save_reward_plot(infos, filename):\n",
    "    plt = create_reward_plot(infos)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.sac import SAC\n",
    "\n",
    "env.uncertainty = 0.2\n",
    "model = SAC.load(\"./best-sac.zip\", env=env, device=\"cuda\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "infos = {}\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, done, _, info = env.step(action)\n",
    "    infos[env.sim_time] = info\n",
    "\n",
    "# render_reward_plot(infos)\n",
    "viz.set_episode_data(env.current_episode)\n",
    "viz.render(env.sim_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4cba6",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d24fa",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb011e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_distances = []\n",
    "\n",
    "viz.reset()\n",
    "max_sim_time = 0.0\n",
    "episodes = 10\n",
    "env.uncertainty = 0.5\n",
    "\n",
    "for i in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, done, _, _ = env.step(action)\n",
    "        \n",
    "    distances = [state.distance for time, state in env.current_episode.get_interceptor(env.current_agent_name).states.all.items()]\n",
    "    best_distances.append(min(distances))\n",
    "\n",
    "    max_sim_time = max(max_sim_time, env.sim_time)\n",
    "    viz.add_episode_data(env.current_episode)\n",
    "\n",
    "best_distance = min(best_distances)\n",
    "worst_distance = max(best_distances)\n",
    "mean_distance = np.mean(best_distances)\n",
    "print(f\"Best Distance: {best_distance:.2f} m | Worst Distance: {worst_distance:.2f} | Mean Distance: {mean_distance:.2f} m\")\n",
    "\n",
    "# render\n",
    "viz.render(max_sim_time)\n",
    "# viz.save_playback(\"sac.gif\", max_sim_time, 5.0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01718af",
   "metadata": {},
   "source": [
    "### Compare with Proportional Navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec7dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 30\n",
    "env.uncertainty = 0.5\n",
    "\n",
    "agent_miss_distances = []\n",
    "prop_nav_miss_distances = []\n",
    "\n",
    "for i in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, done, _, _ = env.step(action)\n",
    "\n",
    "    distances = [state.distance for time, state in env.current_episode.get_interceptor(env.current_agent_name).states.all.items()]\n",
    "    agent_miss_distances.append(min(distances))\n",
    "\n",
    "for i in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = proportional_nav_pilot.step(obs, settings.time_step, interceptor, target)\n",
    "        obs, rewards, done, _, _ = env.step(action)\n",
    "\n",
    "    distances = [state.distance for time, state in env.current_episode.get_interceptor(env.current_agent_name).states.all.items()]\n",
    "    prop_nav_miss_distances.append(min(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_mean_miss_distance = np.mean(agent_miss_distances)\n",
    "prop_nav_mean_miss_distance = np.mean(prop_nav_miss_distances)\n",
    "print(f\"Agent Mean Miss Distance: {agent_mean_miss_distance:.2f} m | Proportional Nav Mean Miss Distance: {prop_nav_mean_miss_distance:.2f} m\")\n",
    "\n",
    "agent_hit_rate = np.mean(np.array(agent_miss_distances) < 50.0)\n",
    "prop_nav_hit_rate = np.mean(np.array(prop_nav_miss_distances) < 50.0)\n",
    "print(f\"Agent Hit Rate: {agent_hit_rate:.2f} | Proportional Nav Hit Rate: {prop_nav_hit_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114aec41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c805f228",
   "metadata": {},
   "source": [
    "## Save and Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, interceptor_state in env.current_episode.interceptors.items():\n",
    "    distances = [state.distance for time , state in interceptor_state.states.all.items()]\n",
    "    best_distance = min(distances)\n",
    "    print (f\"Interceptor {id} best distance: {best_distance:.2f} m\")\n",
    "\n",
    "viz.set_episode_data(env.current_episode)\n",
    "viz.save_playback(\"output.gif\", env.sim_time, 5.0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d32131",
   "metadata": {},
   "source": [
    "## Plot Distance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01710c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all distances from the interceptor to the target\n",
    "distances = {time: state.distance for time, state in env.current_episode.get_interceptor(env.current_agent_name).states.all.items()}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "times = list(distances.keys())\n",
    "distance_values = list(distances.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(times, distance_values, label=\"Distance over Time\")\n",
    "\n",
    "# Find the minimum value and its corresponding time\n",
    "min_distance = min(distance_values)\n",
    "min_time = times[distance_values.index(min_distance)]\n",
    "\n",
    "# Add a label for the minimum value\n",
    "plt.scatter(min_time, min_distance, color='red', label=f\"Min Distance: {min_distance:.2f} m\")\n",
    "plt.text(min_time, min_distance, f\"({min_time:.2f}, {min_distance:.2f})\", color='red', fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Distance (m)\")\n",
    "plt.title(\"Interceptor to Target Distance Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hauptseminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
