{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34f97ef",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47555961",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0616386",
   "metadata": {},
   "source": [
    "## Physical Models of target and interceptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5630312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physics.missile import PhysicalMissleModel\n",
    "import physics.math as physics\n",
    "import numpy as np\n",
    "\n",
    "interceptor_speed = physics.mach_to_ms(4.0)  # Speed of the interceptor in m/s\n",
    "target_speed = physics.mach_to_ms(3.0)  # Speed of the target in m/s\n",
    "\n",
    "# target_init_velocty = np.array([target_speed * 0.707, target_speed * 0.707, 0.0]) # in a 45 degree angle\n",
    "target_init_velocty = np.array([target_speed, 0.0, 0.0])\n",
    "\n",
    "target = PhysicalMissleModel(velocity=target_init_velocty, max_acc=100 * 9.81, pos=np.array([-5_000.0, -5_000.0, 20_000.0]))\n",
    "interceptor = PhysicalMissleModel(velocity=np.array([0.0, 0.0, interceptor_speed]), max_acc=100 * 9.81, pos=np.array([0.0, 0.0, 1000.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3072d3",
   "metadata": {},
   "source": [
    "## Environment & Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.environment import MissileEnv, MissileEnvSettings\n",
    "from pilots.random_evasion_pilot import RandomEvasionPilot\n",
    "from pilots.constant_acc_pilot import ConstantAccelerationPilot\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from visualization.matplot_viz import MatplotVisualizer\n",
    "\n",
    "# target behavior\n",
    "target_pilot = RandomEvasionPilot()\n",
    "# target_pilot = ConstantAccelerationPilot(acc=np.array([0.2, 0.0]))\n",
    "# target_pilot = None # keep trajectory\n",
    "\n",
    "settings = MissileEnvSettings()\n",
    "settings.time_step = 0.1    # Time step for the simulation\n",
    "settings.realtime = False    # Runs faster than real-time\n",
    "settings.time_limit = 50.0  # Time-limit for the episode\n",
    "settings.hit_distance = 100.0  # Distance at which the interceptor is considered to have hit the target\n",
    "env = MissileEnv(settings=settings, target=target, interceptor=interceptor, target_pilot=target_pilot)\n",
    "\n",
    "viz = MatplotVisualizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f42b1a",
   "metadata": {},
   "source": [
    "# Proportional Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32442557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pilots.proportional_nav_pilot import ZemProportionalNavPilot\n",
    "from visualization.matplot_viz import MatplotVisualizer\n",
    "import time\n",
    "\n",
    "\n",
    "# setup pilots\n",
    "proportional_nav_pilot = ZemProportionalNavPilot(max_acc=100*9.81, n=7)\n",
    "\n",
    "max_sim_time = 0.0\n",
    "episodes = 1\n",
    "env.set_uncertainty(0.0)  # Set the uncertainty for the environment\n",
    "env.set_current_agent_name(\"Prop. Nav. Pilot\")\n",
    "\n",
    "viz.reset()\n",
    "\n",
    "for i in range(episodes):\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # get un-normalized observations for the interceptor (environment outputs normalized observations for RL agent)\n",
    "        interceptor_observations = env.get_interceptor_observations(norm=False).pack()\n",
    "        ground_base_observations = env.get_ground_base_observations().pack()\n",
    "\n",
    "        # pilot the interceptor using the proportional navigation algorithm\n",
    "        action_1 = proportional_nav_pilot.step(ground_base_observations, interceptor, settings.time_step, on_board=False)\n",
    "        action_2 = proportional_nav_pilot.step(interceptor_observations, interceptor, settings.time_step, on_board=True)\n",
    "\n",
    "        obs, reward, done, _, _ = env.step(action_2)  # Take a step in the environment\n",
    "\n",
    "    # Record the best distance for this episode\n",
    "    max_sim_time = max(max_sim_time, env.sim_time)\n",
    "    viz.add_episode_data(env.current_episode)\n",
    "\n",
    "\n",
    "viz.render(max_sim_time)\n",
    "# viz.save_playback(\".playbacks/prop-nav.gif\", max_sim_time, 5.0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.save_playback(\"./.playbacks/prop-nav.gif\", max_sim_time, 5.0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73102dc1",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce4163",
   "metadata": {},
   "source": [
    "## Soft Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98343373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "monitored_env = Monitor(env) # required for convergence check in callback\n",
    "\n",
    "# SAC model hyperparameters\n",
    "entropy_coef = \"auto\"  # Automatically adjust entropy coefficient\n",
    "target_entropy = \"auto\"  # Target entropy for the policy\n",
    "device = \"cuda\"    \n",
    "\n",
    "# values by optuna\n",
    "net_arch = dict(pi=6*[288], qf=5*[480])  # Neural network architecture for policy and Q-function\n",
    "learning_rate = 0.00172\n",
    "buffer_size = 7000\n",
    "tau = 0.0258\n",
    "gamma = 0.902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ba891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "\n",
    "model = SAC(\"MlpPolicy\", monitored_env, \n",
    "            verbose=1, \n",
    "            tensorboard_log=\"./.logs/sac\", \n",
    "            device=device, \n",
    "            # learning_rate=learning_rate,\n",
    "            # buffer_size=buffer_size,\n",
    "            # tau=tau,\n",
    "            # gamma=gamma,\n",
    "            policy_kwargs=dict(net_arch=net_arch),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if snapshot file exists\n",
    "snapshot_path = \"./.snapshots/snapshot-sac.zip\"\n",
    "if os.path.exists(snapshot_path):\n",
    "    model = SAC.load(snapshot_path, env=monitored_env, \n",
    "                    device=device, \n",
    "                    learning_rate=learning_rate,\n",
    "                    buffer_size=buffer_size,\n",
    "                    tau=tau,\n",
    "                    gamma=gamma,\n",
    "            )\n",
    "    print(\"Loaded existing model from snapshot.\")\n",
    "else:\n",
    "    print(\"No existing model snapshot found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db72a6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d13b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.convergence_callback import ConvergenceCallback\n",
    "from util.save_on_episode_end import SaveOnEpisodeEnd\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_best_distance(episode):\n",
    "    # get the best distance between the interceptor and the target\n",
    "    return min([state.distance for time, state in episode.get_interceptor(env.current_agent_name).states.all.items()])\n",
    "\n",
    "def eval_model(model, save_gif=False):\n",
    "    eval_distances = []\n",
    "    max_sim_time = 0.0\n",
    "\n",
    "    if save_gif:\n",
    "        viz.reset()\n",
    "\n",
    "    for _ in range(10):\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs = env.get_interceptor_observations(settings.time_step).pack()\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, _, done, _, _ = env.step(action)\n",
    "\n",
    "        best_distance = get_best_distance(env.current_episode)\n",
    "        eval_distances.append(best_distance)\n",
    "\n",
    "        if save_gif:\n",
    "            viz.add_episode_data(env.current_episode)\n",
    "            max_sim_time = max(max_sim_time, env.sim_time)\n",
    "    \n",
    "    mean_distance = np.mean(eval_distances)\n",
    "    print(f\"Mean distance: {mean_distance:.2f} m\")\n",
    "\n",
    "    if save_gif:\n",
    "        viz.save_playback(f\"./.playbacks/last-eval.gif\", max_sim_time, 10.0, 4)\n",
    "\n",
    "    return mean_distance\n",
    "\n",
    "def train_until_miss_distance(model, model_name=\"rl\", distance=100.0):\n",
    "    best_mean_distance = float('inf')\n",
    "\n",
    "    first_run = True\n",
    "    complete = False\n",
    "    while not complete:    \n",
    "        with tqdm(total=20_000, desc=\"Training Progress\", unit=\"steps\") as pbar:\n",
    "            model.learn(total_timesteps=20_000, progress_bar=pbar, reset_num_timesteps=first_run)\n",
    "            first_run = False\n",
    "\n",
    "        mean_distance = eval_model(model, True)\n",
    "        print(f\"Current mean miss distance training: {mean_distance:.2f} m\")\n",
    "\n",
    "        if mean_distance < best_mean_distance:\n",
    "            best_mean_distance = mean_distance\n",
    "            model.save(f\"./.snapshots/snapshot-{model_name}\")\n",
    "            print(f\"New best model saved with mean miss distance: {best_mean_distance:.2f} m\")\n",
    "\n",
    "        if mean_distance < distance:\n",
    "            complete = True\n",
    "            print(f\"Training complete! Mean miss distance: {mean_distance:.2f} m\")\n",
    "\n",
    "def train_until_convergence(model, model_name=\"rl\", reward_variance_threshold=200):\n",
    "    convergence_callback = ConvergenceCallback(10, reward_variance_threshold)\n",
    "    save_model_callback = SaveOnEpisodeEnd(save_dir=\"./.snapshots\", model_name=model_name)\n",
    "\n",
    "    callbacks = CallbackList([convergence_callback, save_model_callback])\n",
    "\n",
    "    max_timesteps = 1_000_000\n",
    "\n",
    "    with tqdm(total=max_timesteps, desc=\"Training Progress\", unit=\"steps\") as pbar:\n",
    "        model.learn(total_timesteps=max_timesteps, callback=callbacks, progress_bar=pbar, reset_num_timesteps=False)\n",
    "\n",
    "    model.save(f\"./.snapshots/converged-{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a572cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env._uncertainty = 0.2\n",
    "total_iterations = 50_000\n",
    "step_size = 5_000\n",
    "\n",
    "first_run = True\n",
    "for i in range(total_iterations // step_size):\n",
    "    print(f\"Training iteration {i + 1}/{total_iterations // step_size}\")\n",
    "    \n",
    "    # Train the model for a step size\n",
    "    model.learn(total_timesteps=step_size, reset_num_timesteps=first_run, progress_bar=True)\n",
    "    first_run = False\n",
    "\n",
    "    # Evaluate the model and output a gif\n",
    "    mean_distance = eval_model(model, True)\n",
    "    print(f\"Mean distance after {i + 1} iterations: {mean_distance:.2f} m\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27663323",
   "metadata": {},
   "source": [
    "### Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env._uncertainty = 0.1\n",
    "train_until_miss_distance(model, \"sac-uncertainty-1\", 200.0)\n",
    "model.save(\"./.snapshots/best-sac\")\n",
    "\n",
    "env._uncertainty = 0.2\n",
    "train_until_miss_distance(model, \"sac-uncertainty-2\", 200.0)\n",
    "model.save(\"./.snapshots/best-sac\")\n",
    "\n",
    "env._uncertainty = 0.3\n",
    "train_until_miss_distance(model, \"sac-uncertainty-3\", 200.0)\n",
    "model.save(\"./.snapshots/best-sac\")\n",
    "\n",
    "env._uncertainty = 0.4\n",
    "train_until_miss_distance(model, \"sac-uncertainty-4\", 200.0)\n",
    "model.save(\"./.snapshots/best-sac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606494c",
   "metadata": {},
   "source": [
    "### Hyperparameter Search (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sample_sac_params(trial):\n",
    "    pi_depth  = trial.suggest_int(\"pi_depth\", 2, 6)\n",
    "    pi_width  = trial.suggest_int(\"pi_width\", 32, 512, step=32)\n",
    "    qf_depth  = trial.suggest_int(\"qf_depth\", 2, 6)\n",
    "    qf_width  = trial.suggest_int(\"qf_width\", 32, 512, step=32)\n",
    "\n",
    "    pi_arch = [pi_width] * pi_depth\n",
    "    qf_arch = [qf_width] * qf_depth\n",
    "\n",
    "    net_arch = dict(pi=pi_arch, qf=qf_arch)\n",
    "    return {\"policy_kwargs\": {\"net_arch\": net_arch}}\n",
    "\n",
    "# 2. Evaluation callback for pruning\n",
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"Callback used for evaluating and reporting a trial.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need.\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "# 3. Objective function\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    total_timesteps = 50_000\n",
    "\n",
    "    monitored_env = Monitor(env)  # Required for convergence check in callback\n",
    "\n",
    "    kwargs = sample_sac_params(trial)\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        monitored_env,\n",
    "        device=\"cuda\",\n",
    "        verbose=0,\n",
    "        tensorboard_log=\"./.logs/sac_optuna\",\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    print(f\"Trial {trial.number} parameters: {kwargs}\")\n",
    "\n",
    "    eval_callback = TrialEvalCallback(monitored_env, trial)\n",
    "\n",
    "    try:\n",
    "        with tqdm(total=total_timesteps, desc=\"Training Progress\", unit=\"steps\") as pbar:\n",
    "            model.learn(total_timesteps=total_timesteps, callback=eval_callback, progress_bar=pbar)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72718e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=10, multivariate=True)\n",
    "pruner = MedianPruner(n_startup_trials=10, n_warmup_steps=1)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler, pruner=pruner)\n",
    "\n",
    "env._uncertainty = 0.0 \n",
    "study.optimize(objective, n_trials=50, timeout=14400 / 2)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf65c8",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe121be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract reward components over time\n",
    "def create_reward_plot(infos):\n",
    "    times = list(infos.keys())\n",
    "    dist_rewards = [info[\"dist-reward\"] for info in infos.values()]\n",
    "    closing_rate_rewards = [info[\"closing-rate-reward\"] for info in infos.values()]\n",
    "    event_rewards = [info[\"event-reward\"] for info in infos.values()]\n",
    "    action_punishments = [info[\"action-punishment\"] for info in infos.values()]\n",
    "    ground_penalties = [info[\"ground-penality\"] for info in infos.values()]\n",
    "    rewards = [info[\"reward\"] for info in infos.values()]\n",
    "\n",
    "    # Create the plot\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(times, dist_rewards, label=\"Distance Reward\")\n",
    "    plt.plot(times, closing_rate_rewards, label=\"Closing Rate Reward\")\n",
    "    plt.plot(times, event_rewards, label=\"Event Reward\")\n",
    "    plt.plot(times, action_punishments, label=\"Action Punishment\")\n",
    "    plt.plot(times, ground_penalties, label=\"Ground Penalty\")\n",
    "    plt.plot(times, rewards, label=\"Total Reward\", linestyle='--', color='black', linewidth=2.5)\n",
    "\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Reward Components\")\n",
    "    plt.title(\"Reward Components Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def render_reward_plot(infos):\n",
    "    plt = create_reward_plot(infos)\n",
    "    plt.show()\n",
    "    plt.show(block=False)\n",
    "\n",
    "def save_reward_plot(infos, filename):\n",
    "    plt = create_reward_plot(infos)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.sac import SAC\n",
    "\n",
    "env._uncertainty = 0.3\n",
    "model = SAC.load(\"./.snapshots/snapshot-sac-uncertainty-2.zip\", env=env, device=\"cuda\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "infos = {}\n",
    "while not done:\n",
    "    # action = proportional_nav_pilot.step(obs, settings.time_step, interceptor, target)\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, done, _, info = env.step(action)\n",
    "    infos[env.sim_time] = info\n",
    "\n",
    "render_reward_plot(infos)\n",
    "# viz.set_episode_data(env.current_episode)\n",
    "# viz.render(env.sim_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4cba6",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d24fa",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb011e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from pilots.proportional_nav_pilot import ZemProportionalNavPilot\n",
    "\n",
    "best_distances = []\n",
    "\n",
    "model = SAC.load(\"./.snapshots/snapshot-sac-uncertainty-3.zip\", env=env, device=\"cuda\")\n",
    "\n",
    "viz.reset()\n",
    "max_sim_time = 0.0\n",
    "episodes = 10\n",
    "env._uncertainty = 0.7\n",
    "\n",
    "acc_rewards = []\n",
    "hits = 0\n",
    "\n",
    "proportional_nav_pilot = ZemProportionalNavPilot(max_acc=100*9.81, n=7)\n",
    "\n",
    "for i in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    acc_reward = 0.0\n",
    "    while not done:\n",
    "        # action, _states = model.predict(obs, deterministic=True)\n",
    "        action = proportional_nav_pilot.step(obs, settings.time_step, env.interceptor, env.target)\n",
    "        obs, rewards, done, _, _ = env.step(action)\n",
    "        acc_reward += rewards\n",
    "        \n",
    "    distances = [state.distance for time, state in env.current_episode.get_interceptor(env.current_agent_name).states.all.items()]\n",
    "    best_distances.append(min(distances))\n",
    "    acc_rewards.append(acc_reward)\n",
    "\n",
    "    max_sim_time = max(max_sim_time, env.sim_time)\n",
    "    viz.add_episode_data(env.current_episode)\n",
    "\n",
    "best_distance = min(best_distances)\n",
    "worst_distance = max(best_distances)\n",
    "mean_distance = np.mean(best_distances)\n",
    "mean_reward = np.mean(acc_rewards)\n",
    "hits = sum([1 for d in best_distances if d < env.settings.hit_distance])\n",
    "\n",
    "hit_rate = hits / episodes * 100.0\n",
    "\n",
    "# render\n",
    "viz.render(max_sim_time)\n",
    "print(f\"Best Distance: {best_distance:.2f} m | Worst Distance: {worst_distance:.2f} | Mean Distance: {mean_distance:.2f} m | Mean Reward: {mean_reward:.2f} | Hits: {hits}/{episodes} | Hit Rate: {hit_rate:.2f}%\")\n",
    "# viz.save_playback(\"sac.gif\", max_sim_time, 5.0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01718af",
   "metadata": {},
   "source": [
    "### Compare with Proportional Navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec7dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 30\n",
    "env._uncertainty = 0.5\n",
    "\n",
    "agent_miss_distances = []\n",
    "prop_nav_miss_distances = []\n",
    "\n",
    "for i in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, done, _, _ = env.step(action)\n",
    "\n",
    "    distances = [state.distance for time, state in env.current_episode.get_interceptor(env.current_agent_name).states.all.items()]\n",
    "    agent_miss_distances.append(min(distances))\n",
    "\n",
    "for i in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = proportional_nav_pilot.step(obs, settings.time_step, interceptor, target)\n",
    "        obs, rewards, done, _, _ = env.step(action)\n",
    "\n",
    "    distances = [state.distance for time, state in env.current_episode.get_interceptor(env.current_agent_name).states.all.items()]\n",
    "    prop_nav_miss_distances.append(min(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_mean_miss_distance = np.mean(agent_miss_distances)\n",
    "prop_nav_mean_miss_distance = np.mean(prop_nav_miss_distances)\n",
    "print(f\"Agent Mean Miss Distance: {agent_mean_miss_distance:.2f} m | Proportional Nav Mean Miss Distance: {prop_nav_mean_miss_distance:.2f} m\")\n",
    "\n",
    "agent_hit_rate = np.mean(np.array(agent_miss_distances) < 50.0)\n",
    "prop_nav_hit_rate = np.mean(np.array(prop_nav_miss_distances) < 50.0)\n",
    "print(f\"Agent Hit Rate: {agent_hit_rate:.2f} | Proportional Nav Hit Rate: {prop_nav_hit_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805f228",
   "metadata": {},
   "source": [
    "## Save and Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, interceptor_state in env.current_episode.interceptors.items():\n",
    "    distances = [state.distance for time , state in interceptor_state.states.all.items()]\n",
    "    best_distance = min(distances)\n",
    "    print (f\"Interceptor {id} best distance: {best_distance:.2f} m\")\n",
    "\n",
    "viz.set_episode_data(env.current_episode)\n",
    "viz.save_playback(\"output.gif\", env.sim_time, 5.0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d32131",
   "metadata": {},
   "source": [
    "## Plot Distance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01710c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all distances from the interceptor to the target\n",
    "distances = {time: state.distance for time, state in env.current_episode.get_interceptor(env.current_agent_name).states.all.items()}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "times = list(distances.keys())\n",
    "distance_values = list(distances.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(times, distance_values, label=\"Distance over Time\")\n",
    "\n",
    "# Find the minimum value and its corresponding time\n",
    "min_distance = min(distance_values)\n",
    "min_time = times[distance_values.index(min_distance)]\n",
    "\n",
    "# Add a label for the minimum value\n",
    "plt.scatter(min_time, min_distance, color='red', label=f\"Min Distance: {min_distance:.2f} m\")\n",
    "plt.text(min_time, min_distance, f\"({min_time:.2f}, {min_distance:.2f})\", color='red', fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Distance (m)\")\n",
    "plt.title(\"Interceptor to Target Distance Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hauptseminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
