{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34f97ef",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47555961",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0616386",
   "metadata": {},
   "source": [
    "## Physical Models of target and interceptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5630312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physics.missile import PhysicalMissleModel\n",
    "import physics.math as physics\n",
    "import numpy as np\n",
    "\n",
    "interceptor_speed = physics.mach_to_ms(4.0)  # Speed of the interceptor in m/s\n",
    "target_speed = physics.mach_to_ms(3.0)  # Speed of the target in m/s\n",
    "\n",
    "target_init_velocty = np.array([target_speed * 0.707, target_speed * 0.707, 0.0]) # in a 45 degree angle\n",
    "\n",
    "target = PhysicalMissleModel(velocity=target_init_velocty, max_acc=100 * 9.81, pos=np.array([-5_000.0, -5_000.0, 20_000.0]))\n",
    "interceptor = PhysicalMissleModel(velocity=np.array([0.0, 0.0, interceptor_speed]), max_acc=100 * 9.81, pos=np.array([0.0, 0.0, 100.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3072d3",
   "metadata": {},
   "source": [
    "## Environment & Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.environment import MissileEnv, MissileEnvSettings\n",
    "from pilots.random_evasion_pilot import RandomEvasionPilot\n",
    "from pilots.constant_acc_pilot import ConstantAccelerationPilot\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# target behavior\n",
    "target_pilot = RandomEvasionPilot()\n",
    "# target_pilot = ConstantAccelerationPilot(acc=np.array([0.2, 0.0]))\n",
    "# target_pilot = None # keep trajectory\n",
    "\n",
    "settings = MissileEnvSettings()\n",
    "settings.time_step = 0.05    # Time step for the simulation\n",
    "settings.realtime = False    # Runs faster than real-time\n",
    "settings.time_limit = 50.0  # Time-limit for the episode\n",
    "env = MissileEnv(settings=settings, target=target, interceptor=interceptor, target_pilot=target_pilot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f42b1a",
   "metadata": {},
   "source": [
    "# Proportional Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67649d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.matplot_viz import MatplotVisualizer\n",
    "\n",
    "viz = MatplotVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32442557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pilots.proportional_nav_pilot import PlanarProportionalNavPilot, ZemProportionalNavPilot, SpaceProportionalNavPilot\n",
    "from visualization.matplot_viz import MatplotVisualizer\n",
    "import time\n",
    "\n",
    "\n",
    "# setup pilots\n",
    "proportional_nav_pilot = ZemProportionalNavPilot(max_acc=100*9.81, n=7)\n",
    "\n",
    "max_sim_time = 0.0\n",
    "episodes = 10\n",
    "env.uncertainty = 0.6\n",
    "env.set_current_agent_name(\"Prop. Nav. Pilot\")\n",
    "\n",
    "viz.reset()\n",
    "\n",
    "for i in range(episodes):\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # get un-normalized observations for the interceptor (environment outputs normalized observations for RL agent)\n",
    "        obs = env.get_interceptor_observations(settings.time_step).pack()\n",
    "\n",
    "        # pilot the interceptor using the proportional navigation algorithm\n",
    "        action = proportional_nav_pilot.step(obs, settings.time_step, interceptor, target)\n",
    "\n",
    "        obs, reward, done, _, _ = env.step(action)  # Take a step in the environment\n",
    "\n",
    "    # Record the best distance for this episode\n",
    "    max_sim_time = max(max_sim_time, env.sim_time)\n",
    "    viz.add_episode_data(env.current_episode)\n",
    "\n",
    "\n",
    "viz.render(max_sim_time)\n",
    "# viz.save_playback(\"prop-nav.gif\", max_sim_time, 5.0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.save_playback(\"./.playbacks/prop-nav.gif\", max_sim_time, 5.0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73102dc1",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce4163",
   "metadata": {},
   "source": [
    "## Soft Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98343373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "monitored_env = Monitor(env) # required for convergence check in callback\n",
    "\n",
    "# SAC model hyperparameters\n",
    "entropy_coef = \"auto\"  # Automatically adjust entropy coefficient\n",
    "target_entropy = \"auto\"  # Target entropy for the policy\n",
    "device = \"cuda\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ba891",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC(\"MlpPolicy\", monitored_env, \n",
    "            verbose=1, \n",
    "            tensorboard_log=\"./.logs/sac\", \n",
    "            device=device, \n",
    "            # ent_coef=entropy_coef, \n",
    "            # target_entropy=target_entropy\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if snapshot file exists\n",
    "snapshot_path = \"./.snapshots/snapshot-sac.zip\"\n",
    "if os.path.exists(snapshot_path):\n",
    "    model = SAC.load(snapshot_path, env=monitored_env, \n",
    "                     device=device, \n",
    "                     # ent_coef=entropy_coef, \n",
    "                     # target_entropy=target_entropy\n",
    "            )\n",
    "    print(\"Loaded existing model from snapshot.\")\n",
    "else:\n",
    "    print(\"No existing model snapshot found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_distance(episode):\n",
    "    # get the best distance between the interceptor and the target\n",
    "    return min([state.distance for time, state in env.current_episode.get_interceptor(env.current_agent_name).states.all.items()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db72a6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d13b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.convergence_callback import ConvergenceCallback\n",
    "from util.save_on_episode_end import SaveOnEpisodeEnd\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "# Train until desired results\n",
    "def eval_model(model):\n",
    "    eval_distances = []\n",
    "    for i in range(10):\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs = env.get_interceptor_observations(settings.time_step).pack()\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, _, done, _, _ = env.step(action)\n",
    "\n",
    "        best_distance = get_best_distance(env.current_episode)\n",
    "        eval_distances.append(best_distance)\n",
    "    \n",
    "    mean_distance = np.mean(eval_distances)\n",
    "    print(f\"Mean distance: {mean_distance:.2f} m\")\n",
    "\n",
    "    return mean_distance\n",
    "\n",
    "def train_until_miss_distance(model, model_name=\"rl\", distance=100.0):\n",
    "    best_mean_distance = float('inf')\n",
    "\n",
    "    complete = False\n",
    "    while not complete:    \n",
    "        with tqdm(total=20_000, desc=\"Training Progress\", unit=\"steps\") as pbar:\n",
    "            model.learn(total_timesteps=20_000, progress_bar=pbar)\n",
    "\n",
    "        mean_distance = eval_model(model)\n",
    "        \n",
    "        if mean_distance < best_mean_distance:\n",
    "            best_mean_distance = mean_distance\n",
    "            model.save(f\"./.snapshots/snapshot-{model_name}\")\n",
    "            print(f\"New best model saved with mean distance: {best_mean_distance:.2f} m\")\n",
    "\n",
    "        if mean_distance < distance:\n",
    "            complete = True\n",
    "            print(f\"Training complete! Mean distance: {mean_distance:.2f} m\")\n",
    "\n",
    "def train_until_convergence(model, model_name=\"rl\", reward_variance_threshold=200):\n",
    "    convergence_callback = ConvergenceCallback(10, reward_variance_threshold)\n",
    "    save_model_callback = SaveOnEpisodeEnd(save_dir=\"./.snapshots\", model_name=model_name)\n",
    "\n",
    "    callbacks = CallbackList([convergence_callback, save_model_callback])\n",
    "\n",
    "    max_timesteps = 1_000_000\n",
    "\n",
    "    with tqdm(total=max_timesteps, desc=\"Training Progress\", unit=\"steps\") as pbar:\n",
    "        model.learn(total_timesteps=max_timesteps, callback=callbacks, progress_bar=pbar)\n",
    "\n",
    "    model.save(f\"./.snapshots/converged-{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27663323",
   "metadata": {},
   "source": [
    "### Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.uncertainty = 0.0\n",
    "train_until_convergence(model, \"sac\", 200)\n",
    "train_until_miss_distance(model, \"sac\", 50.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "def sample_sac_params(trial: optuna.Trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-1),\n",
    "        \"buffer_size\": trial.suggest_int(\"buffer_size\", 1_000, 1_000_000, step=50_000),\n",
    "        \"learning_starts\": trial.suggest_int(\"learning_starts\", 100, 10_000),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512, 1024]),\n",
    "        \"tau\": trial.suggest_float(\"tau\", 1e-3, 0.1),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.9, 0.999),\n",
    "        \"ent_coef\": trial.suggest_float(\"ent_coef\", 0.0, 0.2),\n",
    "        \"gradient_steps\": trial.suggest_int(\"gradient_steps\", 1, 20),\n",
    "    }\n",
    "\n",
    "# 2. Evaluation callback for pruning\n",
    "class TrialEvalCallback(EvalCallback):\n",
    "    def __init__(self, eval_env, trial, n_eval_episodes=5, eval_freq=5_000):\n",
    "        super().__init__(eval_env, eval_freq=eval_freq, n_eval_episodes=n_eval_episodes, verbose=0)\n",
    "        self.trial = trial\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if super()._on_step():\n",
    "            self.trial.report(self.last_mean_reward, self.n_calls)\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# 3. Objective function\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    total_timesteps = 50_000\n",
    "\n",
    "    monitored_env = Monitor(env)  # Required for convergence check in callback\n",
    "\n",
    "    kwargs = sample_sac_params(trial)\n",
    "    model = SAC(\"MlpPolicy\", monitored_env, verbose=0, tensorboard_log=\"./.logs/sac_optuna\", **kwargs)\n",
    "\n",
    "    print(f\"Trial {trial.number} parameters: {kwargs}\")\n",
    "\n",
    "    eval_callback = TrialEvalCallback(monitored_env, trial,\n",
    "                                      n_eval_episodes=5,\n",
    "                                      eval_freq=total_timesteps // 10)\n",
    "\n",
    "    try:\n",
    "        model.learn(total_timesteps=total_timesteps, callback=eval_callback)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72718e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 20:03:10,419] A new study created in memory with name: no-name-6af87f57-7a3b-4cd2-b0b5-cc53ece25699\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_22368\\1448181256.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 parameters: {'learning_rate': 0.00014278668778394503, 'buffer_size': 651000, 'learning_starts': 5267, 'batch_size': 64, 'tau': 0.06552108022966996, 'gamma': 0.984678751716758, 'ent_coef': 0.020140498064401637, 'gradient_steps': 7}\n",
      "Interceptor crashed into the ground\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n",
      "Interceptor crashed into the ground\n",
      "Interceptor crashed into the ground\n",
      "Interceptor crashed into the ground\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n",
      "Interceptor is now in terminal phase\n",
      "Interceptor is now in midcourse phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-06-25 20:19:40,587] Trial 0 failed with parameters: {'learning_rate': 0.00014278668778394503, 'buffer_size': 651000, 'learning_starts': 5267, 'batch_size': 64, 'tau': 0.06552108022966996, 'gamma': 0.984678751716758, 'ent_coef': 0.020140498064401637, 'gradient_steps': 7} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_22368\\1448181256.py\", line 47, in objective\n",
      "    model.learn(total_timesteps=total_timesteps, callback=eval_callback)\n",
      "  File \"c:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\stable_baselines3\\sac\\sac.py\", line 308, in learn\n",
      "    return super().learn(\n",
      "  File \"c:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 347, in learn\n",
      "    self.train(batch_size=self.batch_size, gradient_steps=gradient_steps)\n",
      "  File \"c:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\stable_baselines3\\sac\\sac.py\", line 282, in train\n",
      "    self.actor.optimizer.step()\n",
      "  File \"c:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\torch\\optim\\optimizer.py\", line 485, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"c:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\torch\\optim\\optimizer.py\", line 79, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"c:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\torch\\optim\\adam.py\", line 246, in step\n",
      "    adam(\n",
      "  File \"c:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\torch\\optim\\optimizer.py\", line 147, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\torch\\optim\\adam.py\", line 933, in adam\n",
      "    func(\n",
      "  File \"c:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\torch\\optim\\adam.py\", line 651, in _multi_tensor_adam\n",
      "    torch._foreach_add_(\n",
      "KeyboardInterrupt\n",
      "[W 2025-06-25 20:19:40,591] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m pruner \u001b[38;5;241m=\u001b[39m MedianPruner(n_startup_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampler\u001b[38;5;241m=\u001b[39msampler, pruner\u001b[38;5;241m=\u001b[39mpruner)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\optuna\\study\\study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    252\u001b[0m ):\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[14], line 47\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     42\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m TrialEvalCallback(env, trial,\n\u001b[0;32m     43\u001b[0m                                   n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     44\u001b[0m                                   eval_freq\u001b[38;5;241m=\u001b[39mtotal_timesteps \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:308\u001b[0m, in \u001b[0;36mSAC.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[0;32m    301\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:282\u001b[0m, in \u001b[0;36mSAC.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    281\u001b[0m actor_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Update target networks\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradient_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m             )\n\u001b[1;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\torch\\optim\\adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    237\u001b[0m         group,\n\u001b[0;32m    238\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         state_steps,\n\u001b[0;32m    244\u001b[0m     )\n\u001b[1;32m--> 246\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\torch\\optim\\optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\torch\\optim\\adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 933\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\hauptseminar\\lib\\site-packages\\torch\\optim\\adam.py:651\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mis_compiling() \u001b[38;5;129;01mand\u001b[39;00m device_state_steps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_cpu:\n\u001b[1;32m--> 651\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_state_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[0;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    655\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=10, multivariate=True)\n",
    "pruner = MedianPruner(n_startup_trials=10, n_warmup_steps=1)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler, pruner=pruner)\n",
    "\n",
    "study.optimize(objective, n_trials=50, timeout=3600)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf65c8",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe121be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract reward components over time\n",
    "def create_reward_plot(infos):\n",
    "    times = list(infos.keys())\n",
    "    dist_rewards = [info[\"dist-reward\"] for info in infos.values()]\n",
    "    closing_rate_rewards = [info[\"closing-rate-reward\"] for info in infos.values()]\n",
    "    event_rewards = [info[\"event-reward\"] for info in infos.values()]\n",
    "    action_punishments = [info[\"action-punishment\"] for info in infos.values()]\n",
    "    ground_penalties = [info[\"ground-penality\"] for info in infos.values()]\n",
    "    rewards = [info[\"reward\"] for info in infos.values()]\n",
    "\n",
    "    # Create the plot\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(times, dist_rewards, label=\"Distance Reward\")\n",
    "    plt.plot(times, closing_rate_rewards, label=\"Closing Rate Reward\")\n",
    "    plt.plot(times, event_rewards, label=\"Event Reward\")\n",
    "    plt.plot(times, action_punishments, label=\"Action Punishment\")\n",
    "    plt.plot(times, ground_penalties, label=\"Ground Penalty\")\n",
    "    plt.plot(times, rewards, label=\"Total Reward\", linestyle='--', color='black', linewidth=2.5)\n",
    "\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Reward Components\")\n",
    "    plt.title(\"Reward Components Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def render_reward_plot(infos):\n",
    "    plt = create_reward_plot(infos)\n",
    "    plt.show()\n",
    "    plt.show(block=False)\n",
    "\n",
    "def save_reward_plot(infos, filename):\n",
    "    plt = create_reward_plot(infos)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.sac import SAC\n",
    "\n",
    "env.uncertainty = 0.0\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "infos = {}\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, done, _, info = env.step(action)\n",
    "    infos[env.sim_time] = info\n",
    "\n",
    "# render_reward_plot(infos)\n",
    "viz.set_episode_data(env.current_episode)\n",
    "viz.render(env.sim_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4cba6",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d24fa",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb011e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_distances = []\n",
    "\n",
    "viz.reset()\n",
    "max_sim_time = 0.0\n",
    "episodes = 30\n",
    "\n",
    "for i in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, done, _, _ = env.step(action)\n",
    "        \n",
    "    distances = [state.distance for time, state in env.current_episode.get_interceptor(env.current_agent_name).states.all.items()]\n",
    "    best_distances.append(min(distances))\n",
    "\n",
    "    max_sim_time = max(max_sim_time, env.sim_time)\n",
    "    viz.add_episode_data(env.current_episode)\n",
    "\n",
    "best_distance = min(best_distances)\n",
    "worst_distance = max(best_distances)\n",
    "mean_distance = np.mean(best_distances)\n",
    "print(f\"Best Distance: {best_distance:.2f} m | Worst Distance: {worst_distance:.2f} | Mean Distance: {mean_distance:.2f} m\")\n",
    "\n",
    "# render\n",
    "# viz.render(env.sim_time)\n",
    "viz.save_playback(\"sac.gif\", max_sim_time, 5.0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01718af",
   "metadata": {},
   "source": [
    "### Compare with Proportional Navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec7dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 30\n",
    "env.uncertainty = 0.0\n",
    "\n",
    "agent_miss_distances = []\n",
    "prop_nav_miss_distances = []\n",
    "\n",
    "for i in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, done, _, _ = env.step(action)\n",
    "\n",
    "    distances = [state.distance for time, state in env.current_episode.get_interceptor(env.current_agent_name).states.all.items()]\n",
    "    agent_miss_distances.append(min(distances))\n",
    "\n",
    "for i in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = proportional_nav_pilot.step(obs, settings.time_step, interceptor, target)\n",
    "        obs, rewards, done, _, _ = env.step(action)\n",
    "\n",
    "    distances = [state.distance for time, state in env.current_episode.get_interceptor(env.current_agent_name).states.all.items()]\n",
    "    prop_nav_miss_distances.append(min(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_mean_miss_distance = np.mean(agent_miss_distances)\n",
    "prop_nav_mean_miss_distance = np.mean(prop_nav_miss_distances)\n",
    "print(f\"Agent Mean Miss Distance: {agent_mean_miss_distance:.2f} m | Proportional Nav Mean Miss Distance: {prop_nav_mean_miss_distance:.2f} m\")\n",
    "\n",
    "agent_hit_rate = np.mean(np.array(agent_miss_distances) < 50.0)\n",
    "prop_nav_hit_rate = np.mean(np.array(prop_nav_miss_distances) < 50.0)\n",
    "print(f\"Agent Hit Rate: {agent_hit_rate:.2f} | Proportional Nav Hit Rate: {prop_nav_hit_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114aec41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c805f228",
   "metadata": {},
   "source": [
    "## Save and Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, interceptor_state in env.current_episode.interceptors.items():\n",
    "    distances = [state.distance for time , state in interceptor_state.states.all.items()]\n",
    "    best_distance = min(distances)\n",
    "    print (f\"Interceptor {id} best distance: {best_distance:.2f} m\")\n",
    "\n",
    "viz.set_episode_data(env.current_episode)\n",
    "viz.save_playback(\"output.gif\", env.sim_time, 5.0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d32131",
   "metadata": {},
   "source": [
    "## Plot Distance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01710c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all distances from the interceptor to the target\n",
    "distances = {time: state.distance for time, state in env.current_episode.get_interceptor(\"Agent\").states.all.items()}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "times = list(distances.keys())\n",
    "distance_values = list(distances.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(times, distance_values, label=\"Distance over Time\")\n",
    "\n",
    "# Find the minimum value and its corresponding time\n",
    "min_distance = min(distance_values)\n",
    "min_time = times[distance_values.index(min_distance)]\n",
    "\n",
    "# Add a label for the minimum value\n",
    "plt.scatter(min_time, min_distance, color='red', label=f\"Min Distance: {min_distance:.2f} m\")\n",
    "plt.text(min_time, min_distance, f\"({min_time:.2f}, {min_distance:.2f})\", color='red', fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Distance (m)\")\n",
    "plt.title(\"Interceptor to Target Distance Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hauptseminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
