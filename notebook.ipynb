{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34f97ef",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47555961",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0616386",
   "metadata": {},
   "source": [
    "## Physical Models of target and interceptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5630312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.missile import PhysicalMissleModel\n",
    "import models.physics as physics\n",
    "import numpy as np\n",
    "\n",
    "interceptor_speed = physics.mach_to_ms(4.0)  # Speed of the interceptor in m/s\n",
    "target_speed = physics.mach_to_ms(3.0)  # Speed of the target in m/s\n",
    "target = PhysicalMissleModel(velocity=np.array([0, target_speed, 0.0]), max_acc=100 * 9.81, pos=np.array([0.0, -10_000, 15_000.0]))\n",
    "interceptor = PhysicalMissleModel(velocity=np.array([0.0, 0.0, interceptor_speed]), max_acc=100 * 9.81, pos=np.array([0.0, 0.0, 100.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3072d3",
   "metadata": {},
   "source": [
    "## Environment & Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.environment import MissileEnv, MissileEnvSettings\n",
    "from pilots.random_evasion_pilot import RandomEvasionPilot\n",
    "\n",
    "# target behavior\n",
    "target_pilot = RandomEvasionPilot(aggression=0.2, trajectory_maintainance=0.01)\n",
    "target_pilot = None\n",
    "\n",
    "settings = MissileEnvSettings()\n",
    "settings.time_step = 0.01    # Time step for the simulation\n",
    "settings.realtime = False    # Runs faster than real-time\n",
    "settings.time_limit = 50.0  # Time-limit for the episode\n",
    "env = MissileEnv(settings=settings, target=target, interceptor=interceptor, target_pilot=target_pilot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f42b1a",
   "metadata": {},
   "source": [
    "# Proportional Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32442557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pilots.proportional_nav_pilot import PlanarProportionalNavPilot, SpaceProportionalNavPilot\n",
    "from visualization.matplot_viz import MatplotVisualizer\n",
    "import time\n",
    "\n",
    "\n",
    "# setup pilots\n",
    "proportional_nav_pilot = PlanarProportionalNavPilot(max_acc=100*9.81, max_speed=interceptor_speed, n=3)\n",
    "\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "\n",
    "viz = MatplotVisualizer()\n",
    "viz.set_episode_data(env.current_episode)\n",
    "\n",
    "last_time = time.time()\n",
    "max_render_fps = 10.0\n",
    "\n",
    "while not done:\n",
    "    # get un-normalized observations for the interceptor (environment outputs normalized observations for RL agent)\n",
    "    obs = env.get_interceptor_observations(settings.time_step).pack()\n",
    "\n",
    "    # pilot the interceptor using the proportional navigation algorithm\n",
    "    action = proportional_nav_pilot.step(obs, settings.time_step)\n",
    "    obs, reward, done, _, _ = env.step(action)  # Take a step in the environment\n",
    "    \n",
    "# viz.playback(env.sim_time, speed=5.0)\n",
    "viz.render(env.sim_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73102dc1",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce4163",
   "metadata": {},
   "source": [
    "## Soft Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98343373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./.logs/sac\", device=\"cuda\")\n",
    "with tqdm(total=100_000, desc=\"Training Progress\", unit=\"steps\") as pbar:\n",
    "    model.learn(total_timesteps=100_000, progress_bar=pbar)\n",
    "\n",
    "viz.set_episode_data(env.current_episode)\n",
    "viz.render(env.sim_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_distance(episode):\n",
    "    # get the best distance between the interceptor and the target\n",
    "    return min([state.distance for time, state in env.current_episode.get_interceptor(\"Agent\").states.all.items()])\n",
    "\n",
    "print(f\"Best distance: {get_best_distance(env.current_episode)} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d13b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train until desired results\n",
    "complete = False\n",
    "while not complete:\n",
    "    with tqdm(total=10_000, desc=\"Training Progress\", unit=\"steps\") as pbar:\n",
    "        model.learn(total_timesteps=10_000, progress_bar=pbar)\n",
    "\n",
    "    eval_distances = []\n",
    "    for i in range(10):\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs = env.get_interceptor_observations(settings.time_step).pack()\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        best_distance = min([state.distance for time, state in env.current_episode.get_interceptor(\"Agent\").states.all.items()])\n",
    "        eval_distances.append(best_distance)\n",
    "    \n",
    "    mean_distance = np.mean(eval_distances)\n",
    "    print(f\"Mean distance: {mean_distance:.2f} m\")\n",
    "    \n",
    "    if mean_distance < 100.0:\n",
    "        complete = True\n",
    "        print(f\"Training complete! Mean distance: {mean_distance:.2f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, done, _, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4cba6",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d24fa",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb011e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_distances = []\n",
    "\n",
    "for i in range(10):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, done, _, _ = env.step(action)\n",
    "        \n",
    "    distances = [state.distance for time, state in env.current_episode.get_interceptor(\"Agent\").states.all.items()]\n",
    "    best_distances.append(min(distances))\n",
    "\n",
    "best_distance = min(best_distances)\n",
    "print(f\"Best distance: {best_distance:.2f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805f228",
   "metadata": {},
   "source": [
    "## Save and Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, interceptor_state in env.current_episode.interceptors.items():\n",
    "    distances = [state.distance for time , state in interceptor_state.states.all.items()]\n",
    "    best_distance = min(distances)\n",
    "    print (f\"Interceptor {id} best distance: {best_distance:.2f} m\")\n",
    "\n",
    "viz.set_episode_data(env.current_episode)\n",
    "viz.save_playback(\"output.gif\", env.sim_time, 5.0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d32131",
   "metadata": {},
   "source": [
    "## Plot Distance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01710c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all distances from the interceptor to the target\n",
    "distances = {time: state.distance for time, state in env.current_episode.get_interceptor(\"Agent\").states.all.items()}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "times = list(distances.keys())\n",
    "distance_values = list(distances.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(times, distance_values, label=\"Distance over Time\")\n",
    "\n",
    "# Find the minimum value and its corresponding time\n",
    "min_distance = min(distance_values)\n",
    "min_time = times[distance_values.index(min_distance)]\n",
    "\n",
    "# Add a label for the minimum value\n",
    "plt.scatter(min_time, min_distance, color='red', label=f\"Min Distance: {min_distance:.2f} m\")\n",
    "plt.text(min_time, min_distance, f\"({min_time:.2f}, {min_distance:.2f})\", color='red', fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Distance (m)\")\n",
    "plt.title(\"Interceptor to Target Distance Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hauptseminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
