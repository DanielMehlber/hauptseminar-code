{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34f97ef",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47555961",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0616386",
   "metadata": {},
   "source": [
    "## Physical Models of target and interceptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5630312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.missile import PhysicalMissleModel\n",
    "import models.physics as physics\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "interceptor_speed = physics.mach_to_ms(5.0)  # Speed of the interceptor in m/s\n",
    "target_speed = physics.mach_to_ms(4.0)  # Speed of the target in m/s\n",
    "target = PhysicalMissleModel(velocity=np.array([target_speed / 2, target_speed / 2, 0.0]), max_acc=100 * 9.81, pos=np.array([0.0, -5_000.0, 20_000.0]))\n",
    "interceptor = PhysicalMissleModel(velocity=np.array([0.0, 0.0, interceptor_speed]), max_acc=100 * 9.81, pos=np.array([0.0, 0.0, 100.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3072d3",
   "metadata": {},
   "source": [
    "## Environment & Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.environment import MissileEnv, MissileEnvSettings\n",
    "from pilots.random_evasion_pilot import RandomEvasionPilot\n",
    "from pilots.constant_acc_pilot import ConstantAccelerationPilot\n",
    "\n",
    "# target behavior\n",
    "target_pilot = RandomEvasionPilot(aggression=0.6, trajectory_maintainance=0.2)\n",
    "# target_pilot = ConstantAccelerationPilot(acc=np.array([0.2, 0.0]))\n",
    "# target_pilot = None # keep trajectory\n",
    "\n",
    "settings = MissileEnvSettings()\n",
    "settings.time_step = 0.05    # Time step for the simulation\n",
    "settings.realtime = False    # Runs faster than real-time\n",
    "settings.time_limit = 50.0  # Time-limit for the episode\n",
    "env = MissileEnv(settings=settings, target=target, interceptor=interceptor, target_pilot=target_pilot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f42b1a",
   "metadata": {},
   "source": [
    "# Proportional Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67649d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.matplot_viz import MatplotVisualizer\n",
    "\n",
    "viz = MatplotVisualizer()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32442557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pilots.proportional_nav_pilot import PlanarProportionalNavPilot, ZemProportionalNavPilot, SpaceProportionalNavPilot\n",
    "from visualization.matplot_viz import MatplotVisualizer\n",
    "import time\n",
    "\n",
    "\n",
    "# setup pilots\n",
    "proportional_nav_pilot = SpaceProportionalNavPilot(max_acc=100*9.81, n=40)\n",
    "\n",
    "\n",
    "done = False\n",
    "obs, _ = env.reset()\n",
    "\n",
    "last_time = time.time()\n",
    "max_render_fps = 10.0\n",
    "\n",
    "episode_distances = []\n",
    "max_sim_time = 0.0\n",
    "\n",
    "episodes = 1\n",
    "\n",
    "viz.reset()\n",
    "\n",
    "for i in range(episodes):\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    while not done:\n",
    "        # get un-normalized observations for the interceptor (environment outputs normalized observations for RL agent)\n",
    "        obs = env.get_interceptor_observations(settings.time_step).pack()\n",
    "\n",
    "        # pilot the interceptor using the proportional navigation algorithm\n",
    "        action = proportional_nav_pilot.step(obs, settings.time_step, interceptor, target)\n",
    "\n",
    "        obs, reward, done, _, _ = env.step(action)  # Take a step in the environment\n",
    "\n",
    "    # Record the best distance for this episode\n",
    "    max_sim_time = max(max_sim_time, env.sim_time)\n",
    "    distances = [state.distance for time, state in env.current_episode.get_interceptor(\"Agent\").states.all.items()]\n",
    "    episode_distances.append(min(distances))\n",
    "\n",
    "    viz.add_episode_data(env.current_episode)\n",
    "\n",
    "print(f\"Best distances over episodes: {episode_distances}\")\n",
    "print(f\"Mean best distance: {np.mean(episode_distances):.2f} m\")\n",
    "# viz.render(max_sim_time)\n",
    "viz.save_playback(\"prop-nav.gif\", max_sim_time, 5.0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73102dc1",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce4163",
   "metadata": {},
   "source": [
    "## Soft Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98343373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./.logs/sac\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_distance(episode):\n",
    "    # get the best distance between the interceptor and the target\n",
    "    return min([state.distance for time, state in env.current_episode.get_interceptor(\"Agent\").states.all.items()])\n",
    "\n",
    "print(f\"Best distance: {get_best_distance(env.current_episode)} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db72a6",
   "metadata": {},
   "source": [
    "### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d13b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train until desired results\n",
    "def eval_model():\n",
    "    eval_distances = []\n",
    "    for i in range(10):\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs = env.get_interceptor_observations(settings.time_step).pack()\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, _, done, _, _ = env.step(action)\n",
    "\n",
    "        best_distance = get_best_distance(env.current_episode)\n",
    "        eval_distances.append(best_distance)\n",
    "    \n",
    "    mean_distance = np.mean(eval_distances)\n",
    "    print(f\"Mean distance: {mean_distance:.2f} m\")\n",
    "\n",
    "    return mean_distance\n",
    "\n",
    "best_mean_distance = float('inf')\n",
    "complete = False\n",
    "\n",
    "while not complete:    \n",
    "    with tqdm(total=20_000, desc=\"Training Progress\", unit=\"steps\") as pbar:\n",
    "        model.learn(total_timesteps=20_000, progress_bar=pbar)\n",
    "\n",
    "    mean_distance = eval_model()\n",
    "    \n",
    "    if mean_distance < best_mean_distance:\n",
    "        best_mean_distance = mean_distance\n",
    "        model.save(f\"best-sac\")\n",
    "        print(f\"New best model saved with mean distance: {best_mean_distance:.2f} m\")\n",
    "\n",
    "    model.save(f\"latest-sac-{int(mean_distance)}\")\n",
    "    viz.set_episode_data(env.current_episode)\n",
    "\n",
    "    if mean_distance < 50.0:\n",
    "        complete = True\n",
    "        print(f\"Training complete! Mean distance: {mean_distance:.2f} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf65c8",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe121be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract reward components over time\n",
    "def create_reward_plot(infos):\n",
    "    times = list(infos.keys())\n",
    "    dist_rewards = [info[\"dist-reward\"] for info in infos.values()]\n",
    "    closing_rate_rewards = [info[\"closing-rate-reward\"] for info in infos.values()]\n",
    "    event_rewards = [info[\"event-reward\"] for info in infos.values()]\n",
    "    action_punishments = [info[\"action-punishment\"] for info in infos.values()]\n",
    "    ground_penalties = [info[\"ground-penality\"] for info in infos.values()]\n",
    "    rewards = [info[\"reward\"] for info in infos.values()]\n",
    "\n",
    "    # Create the plot\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(times, dist_rewards, label=\"Distance Reward\")\n",
    "    plt.plot(times, closing_rate_rewards, label=\"Closing Rate Reward\")\n",
    "    plt.plot(times, event_rewards, label=\"Event Reward\")\n",
    "    plt.plot(times, action_punishments, label=\"Action Punishment\")\n",
    "    plt.plot(times, ground_penalties, label=\"Ground Penalty\")\n",
    "    plt.plot(times, rewards, label=\"Total Reward\", linestyle='--', color='black', linewidth=2.5)\n",
    "\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Reward Components\")\n",
    "    plt.title(\"Reward Components Over Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def render_reward_plot(infos):\n",
    "    plt = create_reward_plot(infos)\n",
    "    plt.show()\n",
    "    plt.show(block=False)\n",
    "\n",
    "def save_reward_plot(infos, filename):\n",
    "    plt = create_reward_plot(infos)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model = SAC.load(\"best-sac\", env=env)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "infos = {}\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, done, _, info = env.step(action)\n",
    "    infos[env.sim_time] = info\n",
    "\n",
    "render_reward_plot(infos)\n",
    "viz.set_episode_data(env.current_episode)\n",
    "# viz.render(env.sim_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4cba6",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d24fa",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb011e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_distances = []\n",
    "\n",
    "viz.reset()\n",
    "max_sim_time = 0.0\n",
    "episodes = 1\n",
    "\n",
    "for i in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, done, _, _ = env.step(action)\n",
    "        \n",
    "    distances = [state.distance for time, state in env.current_episode.get_interceptor(\"Agent\").states.all.items()]\n",
    "    best_distances.append(min(distances))\n",
    "\n",
    "    max_sim_time = max(max_sim_time, env.sim_time)\n",
    "    viz.add_episode_data(env.current_episode)\n",
    "\n",
    "best_distance = min(best_distances)\n",
    "worst_distance = max(best_distances)\n",
    "mean_distance = np.mean(best_distances)\n",
    "print(f\"Best Distance: {best_distance:.2f} m | Worst Distance: {worst_distance:.2f} | Mean Distance: {mean_distance:.2f} m\")\n",
    "\n",
    "# render\n",
    "# viz.render(env.sim_time)\n",
    "viz.save_playback(\"sac.gif\", max_sim_time, 5.0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805f228",
   "metadata": {},
   "source": [
    "## Save and Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, interceptor_state in env.current_episode.interceptors.items():\n",
    "    distances = [state.distance for time , state in interceptor_state.states.all.items()]\n",
    "    best_distance = min(distances)\n",
    "    print (f\"Interceptor {id} best distance: {best_distance:.2f} m\")\n",
    "\n",
    "viz.set_episode_data(env.current_episode)\n",
    "viz.save_playback(\"output.gif\", env.sim_time, 5.0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d32131",
   "metadata": {},
   "source": [
    "## Plot Distance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01710c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all distances from the interceptor to the target\n",
    "distances = {time: state.distance for time, state in env.current_episode.get_interceptor(\"Agent\").states.all.items()}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "times = list(distances.keys())\n",
    "distance_values = list(distances.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(times, distance_values, label=\"Distance over Time\")\n",
    "\n",
    "# Find the minimum value and its corresponding time\n",
    "min_distance = min(distance_values)\n",
    "min_time = times[distance_values.index(min_distance)]\n",
    "\n",
    "# Add a label for the minimum value\n",
    "plt.scatter(min_time, min_distance, color='red', label=f\"Min Distance: {min_distance:.2f} m\")\n",
    "plt.text(min_time, min_distance, f\"({min_time:.2f}, {min_distance:.2f})\", color='red', fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Distance (m)\")\n",
    "plt.title(\"Interceptor to Target Distance Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hauptseminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
